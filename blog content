$ helm repo add enapter https://enapter.github.io/charts/
"enapter" has been added to your repositories
$ helm pull enapter/keydb --version 0.48.0
Extract the folders from the compressed file.

Make the following changes in values.yaml:
Add the multisite block at line 11:
multisite:
  enabled: true 
  connection_string: "--replicaof serverip port" #if more sites are there then add more like this "--replicaof <siteA-IP> <siteA-port> --replicaof <siteB-IP> <siteB-port>"
Since we are adding a Nodeport service add the port for that service as well at line 18:
exposeNodePort: 32333 #value should be 32333 in site-a values.yaml and 32334 in site-b values.yaml
Comment out the line at line 132:
#readinessProbeRandomUuid: "90f717dd-0e68-43b8-9363-fddaad00d6c9"
Set line 170 to storageClass: "" and comment the line 173

#need to test the storageclass changes and see if persistent volumes are getting created and are working fine. Also need to change serverip to actual ip address

Once we made the above changes in values.yaml, copy this file and create a new values.yaml. This is because there are 2 sites and we need to change the connection_string in both of them. Now, rename the first values.yaml to sitea-values.yaml and the second one to siteb-values.yaml. 32333 on site-a and 32334 on site-b are the nodeports we are exposing on the nodes and we can use either the site-ip or hostname if available. For this example, we are going ahead with the ip.
In sitea-values.yaml:
multisite:
  enabled: true 
  connection_string: "--replicaof siteb-ip 32334"

In siteb-values.yaml:
multisite:
  enabled: true 
  connection_string: "--replicaof sitea-ip 32333"

Remove the "loading_response" line and lets make the changes so that the cm-health.yaml looks like this.

Add the below block in secret-utils.yaml in line 22:
{{- if .Values.multisite.enabled  }}
    replicas+=("{{ .Values.multisite.connection_string }}")
{{- end }}

Remove the following lines from sts.yaml in line 158:
        - name: REDIS_EXPORTER_WEB_LISTEN_ADDRESS
          value: "0.0.0.0:{{ .Values.exporter.port }}"
        - name: REDIS_EXPORTER_WEB_TELEMETRY_PATH
          value: {{ .Values.exporter.scrapePath | quote }}
and add the below lines:
- name: REDIS_PORT
  value: {{ .Values.internalPort | quote }}
In line 278 make sure the emptyDir: {} 

Since we are enabling multisite communication through a different service we need to add that service (NodePort) spec to svc.yaml:
---
{{- if .Values.multisite.enabled  }}
apiVersion: v1
kind: Service
metadata:
  name: {{ include "keydb.fullname" . }}-multisite
  labels:
    {{- include "keydb.labels" . | nindent 4 }}
  annotations:
    {{- toYaml .Values.service.annotations | nindent 4 }}
spec:
  type: NodePort
  ports:
  - name: {{ .Values.portName | quote }}
    port: {{ .Values.port | int }}
    protocol: TCP
    targetPort: keydb
    nodePort: {{ .Values.exposeNodePort }}
    {{- if .Values.service.appProtocol.enabled }}
    appProtocol: redis
    {{- end }}
  - name: {{ .Values.exporter.portName | quote }}
    port: {{ .Values.exporter.port | int }}
    protocol: TCP
    targetPort: {{ .Values.exporter.portName | quote }}
    {{- if .Values.service.appProtocol.enabled }}
    appProtocol: http
    {{- end }}
  selector:
    {{- include "keydb.selectorLabels" . | nindent 4 }}
  sessionAffinity: ClientIP
{{- end }}

We will be using the kind clusters in this blog, we will be using the same cluster configurations from this blog. 

Now that we have done the changes in our values.yaml and clusters are ready, lets deploy the chart to our kubernetes sites with its own values.yaml:
$ kubectl config use-context kind-sitea
Switched to context "kind-sitea".
$ helm install keydb-a -f sitea-values.yaml .
NAME: keydb-a
LAST DEPLOYED: Mon Dec 25 22:12:54 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
$ kubectl get all
NAME            READY   STATUS    RESTARTS   AGE
pod/keydb-a-0   1/1     Running   0          2m4s
pod/keydb-a-1   1/1     Running   0          95s
pod/keydb-a-2   1/1     Running   0          71s

NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/keydb-a             ClusterIP   10.96.211.222   <none>        6379/TCP,9121/TCP               2m4s
service/keydb-a-headless    ClusterIP   None            <none>        6379/TCP                        2m4s
service/keydb-a-multisite   NodePort    10.96.186.24    <none>        6379:32333/TCP,9121:31036/TCP   2m4s
service/kubernetes          ClusterIP   10.96.0.1       <none>        443/TCP                         23m

NAME                       READY   AGE
statefulset.apps/keydb-a   3/3     2m4s
$ kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS   REASON   AGE
persistentvolume/pvc-57876906-9bdf-4428-8126-028c6ad982e3   1Gi        RWO            Delete           Bound    default/keydb-data-keydb-a-1   standard                116s
persistentvolume/pvc-7d194092-4272-4a61-b737-98b8de142515   1Gi        RWO            Delete           Bound    default/keydb-data-keydb-a-2   standard                92s
persistentvolume/pvc-b5b1d178-8d38-40ba-a726-81559e5c2090   1Gi        RWO            Delete           Bound    default/keydb-data-keydb-a-0   standard                2m26s

NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/keydb-data-keydb-a-0   Bound    pvc-b5b1d178-8d38-40ba-a726-81559e5c2090   1Gi        RWO            standard       2m28s
persistentvolumeclaim/keydb-data-keydb-a-1   Bound    pvc-57876906-9bdf-4428-8126-028c6ad982e3   1Gi        RWO            standard       119s
persistentvolumeclaim/keydb-data-keydb-a-2   Bound    pvc-7d194092-4272-4a61-b737-98b8de142515   1Gi        RWO            standard       95s

The keydb pod is continously trying to connect to the keydb pod in site-b
1:22:S 25 Dec 2023 16:45:54.102 * Connecting to MASTER siteb-ip:32334
1:22:S 25 Dec 2023 16:45:54.104 * Unable to connect to MASTER: Resource temporarily unavailable

Let's deploy the keydb helm chart in site-b cluster as well:

$ kubectl config use-context kind-siteb
Switched to context "kind-siteb".

$ helm install keydb-b -f siteb-values.yaml .
NAME: keydb-b
LAST DEPLOYED: Mon Dec 25 22:20:38 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None

$ kubectl get all 
NAME            READY   STATUS    RESTARTS   AGE
pod/keydb-b-0   1/1     Running   0          96s
pod/keydb-b-1   1/1     Running   0          62s
pod/keydb-b-2   1/1     Running   0          27s

NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
service/keydb-b             ClusterIP   10.96.219.211   <none>        6379/TCP,9121/TCP               96s
service/keydb-b-headless    ClusterIP   None            <none>        6379/TCP                        96s
service/keydb-b-multisite   NodePort    10.96.92.65     <none>        6379:32334/TCP,9121:31814/TCP   96s
service/kubernetes          ClusterIP   10.96.0.1       <none>        443/TCP                         27m

NAME                       READY   AGE
statefulset.apps/keydb-b   3/3     96s

$ kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS   REASON   AGE
persistentvolume/pvc-22755017-38d3-4231-8274-da814e5725c5   1Gi        RWO            Delete           Bound    default/keydb-data-keydb-b-2   standard                47s
persistentvolume/pvc-4ec9bc94-d6dd-465e-8e2a-1db1567f750f   1Gi        RWO            Delete           Bound    default/keydb-data-keydb-b-1   standard                82s
persistentvolume/pvc-92745a78-db36-4cef-8c19-56d88663d0c9   1Gi        RWO            Delete           Bound    default/keydb-data-keydb-b-0   standard                117s

NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/keydb-data-keydb-b-0   Bound    pvc-92745a78-db36-4cef-8c19-56d88663d0c9   1Gi        RWO            standard       2m
persistentvolumeclaim/keydb-data-keydb-b-1   Bound    pvc-4ec9bc94-d6dd-465e-8e2a-1db1567f750f   1Gi        RWO            standard       86s
persistentvolumeclaim/keydb-data-keydb-b-2   Bound    pvc-22755017-38d3-4231-8274-da814e5725c5   1Gi        RWO            standard       51s


Test the replication between both sites:
If you check the logs of the keydb pods then there is lot of info along with the following line which indicates that the sync is successful:
1:24:S 25 Dec 2023 16:57:35.735 * MASTER <-> REPLICA sync: Finished with success

Now, in site-a cluster:
$ kubectl exec -it keydb-a-0 -- keydb-cli
Message of the day:
  KeyDB has now joined Snap! See the announcement at:  https://docs.keydb.dev/news

127.0.0.1:6379> keys *
(empty array)
127.0.0.1:6379> set foo bar
OK
127.0.0.1:6379> keys *
1) "foo"
127.0.0.1:6379> get foo
"bar"

In site-b, we can see that key is already synced. 
$ kubectl exec -it keydb-b-0 -- keydb-cli
Message of the day:
  KeyDB has now joined Snap! See the announcement at:  https://docs.keydb.dev/news

127.0.0.1:6379> keys *
1) "foo"
127.0.0.1:6379> get foo
"bar"

The sync not only happens in between 2 clusters but also in between all the 3 pods within the cluster.